{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyltp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d8a53d8e1387>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyltp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyltp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSegmentor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyltp'"
     ]
    }
   ],
   "source": [
    "import pyltp\n",
    "from gensim.models import Word2Vec\n",
    "from pyltp import Segmentor\n",
    "import jieba\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from pyltp import  SentenceSplitter,NamedEntityRecognizer,Postagger,Parser,Segmentor\n",
    "from gensim import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"news_corpus\",\"r\") as f:\n",
    "    news = ''\n",
    "    for line in f.readlines():\n",
    "        news += line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cws_model = \"ltp_data_v3.4.0/cws.model\"\n",
    "pos_model = \"ltp_data_v3.4.0/pos.model\"\n",
    "par_model = \"ltp_data_v3.4.0/parser.model\"\n",
    "ner_model = \"ltp_data_v3.4.0/ner.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_list(sentence,model):\n",
    "    #得到分词\n",
    "    segmentor = Segmentor()\n",
    "    segmentor.load(model)\n",
    "    word_list = list(segmentor.segment(sentence))\n",
    "    segmentor.release()\n",
    "    return word_list\n",
    "\n",
    "def get_postag_list(word_list,model):\n",
    "    #得到词性标注\n",
    "    postag = Postagger()\n",
    "    postag.load(model)\n",
    "    postag_list = list(postag.postag(word_list))\n",
    "    postag.release()\n",
    "    return postag_list\n",
    "\n",
    "def get_parser_list(word_list,postag_list,model):\n",
    "    #得到依存关系\n",
    "    parser = Parser()\n",
    "    parser.load(model)\n",
    "    arcs = parser.parse(word_list,postag_list)\n",
    "    arc_list = [(arc.head,arc.relation) for arc in arcs]\n",
    "    parser.release()\n",
    "    return arc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = get_word_list(news,cws_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "postag_list = get_postag_list(word_list,pos_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_list = get_parser_list(word_list,postag_list,par_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "据 (18, 'ADV')\n",
      "彭博社 (4, 'SBV')\n",
      "15日 (4, 'ADV')\n",
      "报道 (1, 'POB')\n",
      "， (1, 'WP')\n",
      "特朗普 (18, 'SBV')\n",
      "正在 (18, 'ADV')\n",
      "开始 (18, 'ADV')\n",
      "为 (18, 'ADV')\n",
      "他 (14, 'ATT')\n",
      "的 (10, 'RAD')\n",
      "2020年 (14, 'ATT')\n",
      "竞选 (14, 'ATT')\n",
      "活动 (9, 'POB')\n",
      "做 (9, 'POB')\n",
      "准备 (15, 'VOB')\n",
      "。特朗普 (16, 'VOB')\n",
      "表示 (0, 'HED')\n",
      ": (18, 'WP')\n",
      "“ (32, 'WP')\n",
      "如果 (32, 'ADV')\n",
      "2020年 (32, 'ADV')\n",
      "除了 (32, 'ADV')\n",
      "我 (25, 'ATT')\n",
      "之外 (27, 'ATT')\n",
      "的 (25, 'RAD')\n",
      "任何人 (28, 'SBV')\n",
      "接管 (23, 'POB')\n",
      "， (23, 'WP')\n",
      "将 (32, 'ADV')\n",
      "会 (32, 'ADV')\n",
      "出现 (18, 'VOB')\n",
      "前所未有 (35, 'ATT')\n",
      "的 (33, 'RAD')\n",
      "股市 (36, 'SBV')\n",
      "崩盘 (32, 'VOB')\n",
      "! (32, 'WP')\n",
      "” (32, 'WP')\n",
      "特朗普 (40, 'SBV')\n",
      "说 (32, 'COO')\n",
      "： (40, 'WP')\n",
      "“ (43, 'WP')\n",
      "好 (40, 'VOB')\n",
      "吧 (43, 'RAD')\n",
      "， (43, 'WP')\n",
      "我 (47, 'SBV')\n",
      "想 (43, 'COO')\n",
      "我 (51, 'ATT')\n",
      "伟大 (51, 'ATT')\n",
      "的 (49, 'RAD')\n",
      "飞行员 (53, 'SBV')\n",
      "会 (53, 'ADV')\n",
      "知道 (47, 'VOB')\n",
      "的 (53, 'RAD')\n",
      "。 (18, 'WP')\n",
      "他们 (57, 'SBV')\n",
      "看到 (59, 'ATT')\n",
      "的 (57, 'RAD')\n",
      "情况 (63, 'SBV')\n",
      "与 (63, 'ADV')\n",
      "过去 (60, 'POB')\n",
      "略 (63, 'ADV')\n",
      "有 (18, 'COO')\n",
      "不同 (63, 'VOB')\n",
      "。 (18, 'WP')\n",
      "我们 (69, 'SBV')\n",
      "会 (69, 'ADV')\n",
      "去 (69, 'ADV')\n",
      "看 (18, 'COO')\n",
      "的 (69, 'RAD')\n",
      "。 (18, 'WP')\n",
      "” (18, 'WP')\n",
      "菲律宾 (74, 'ATT')\n",
      "总统 (75, 'ATT')\n",
      "杜特尔特 (77, 'SBV')\n",
      "日前 (77, 'ADV')\n",
      "表示 (18, 'COO')\n",
      "， (77, 'WP')\n",
      "美国 (80, 'SBV')\n",
      "希望 (77, 'VOB')\n",
      "战争 (83, 'SBV')\n",
      "一直 (83, 'ADV')\n",
      "持续 (80, 'VOB')\n",
      "， (80, 'WP')\n",
      "这样 (90, 'ADV')\n",
      "其他 (87, 'ATT')\n",
      "国家 (90, 'SBV')\n",
      "就 (89, 'ADV')\n",
      "会 (90, 'ADV')\n",
      "购买 (80, 'COO')\n",
      "美国 (93, 'ATT')\n",
      "的 (91, 'RAD')\n",
      "飞机 (90, 'VOB')\n",
      "、 (95, 'WP')\n",
      "军舰 (93, 'COO')\n",
      "和 (97, 'LAD')\n",
      "子弹 (93, 'COO')\n",
      "。 (77, 'WP')\n",
      "一旦 (101, 'ADV')\n",
      "战争 (101, 'SBV')\n",
      "结束 (77, 'COO')\n",
      "， (101, 'WP')\n",
      "美国 (105, 'SBV')\n",
      "将 (105, 'ADV')\n",
      "有 (101, 'COO')\n",
      "一 (108, 'ATT')\n",
      "大 (108, 'ATT')\n",
      "群 (109, 'ATT')\n",
      "人 (105, 'DBL')\n",
      "因此 (111, 'ADV')\n",
      "失业 (105, 'VOB')\n",
      "， (111, 'WP')\n",
      "并且 (117, 'ADV')\n",
      "无法 (117, 'ADV')\n",
      "在 (117, 'ADV')\n",
      "国外 (115, 'POB')\n",
      "销售 (111, 'COO')\n",
      "任何 (119, 'ATT')\n",
      "产品 (117, 'VOB')\n",
      "。 (18, 'WP')\n",
      "杜特尔特 (127, 'SBV')\n",
      "在 (127, 'ADV')\n",
      "一 (124, 'ATT')\n",
      "次 (125, 'ATT')\n",
      "演讲 (126, 'ATT')\n",
      "中 (122, 'POB')\n",
      "说 (18, 'COO')\n",
      "， (127, 'WP')\n",
      "美国 (131, 'SBV')\n",
      "无法 (131, 'ADV')\n",
      "承受 (127, 'VOB')\n",
      "战争 (134, 'ATT')\n",
      "的 (132, 'RAD')\n",
      "结束 (131, 'VOB')\n",
      "， (131, 'WP')\n",
      "因为 (145, 'ADV')\n",
      "那样 (139, 'ATT')\n",
      "的 (137, 'RAD')\n",
      "话 (136, 'POB')\n",
      "， (136, 'WP')\n",
      "很多 (143, 'ATT')\n",
      "美国 (143, 'ATT')\n",
      "人 (145, 'SBV')\n",
      "将 (145, 'ADV')\n",
      "失去 (131, 'COO')\n",
      "工作 (145, 'VOB')\n",
      ", (145, 'WP')\n",
      "美国 (149, 'SBV')\n",
      "希望 (145, 'COO')\n",
      "出售 (149, 'VOB')\n",
      "飞机 (150, 'VOB')\n",
      "、 (153, 'WP')\n",
      "轮船 (151, 'COO')\n",
      "和 (155, 'LAD')\n",
      "子弹 (151, 'COO')\n",
      "。 (18, 'WP')\n",
      "这 (158, 'SBV')\n",
      "就是 (18, 'COO')\n",
      "美国 (160, 'SBV')\n",
      "希望 (158, 'VOB')\n",
      "一直 (163, 'ADV')\n",
      "能 (163, 'ADV')\n",
      "有 (160, 'VOB')\n",
      "战争 (163, 'VOB')\n",
      "， (163, 'WP')\n",
      "来 (167, 'ADV')\n",
      "保证 (163, 'COO')\n",
      "美国 (169, 'ATT')\n",
      "人 (172, 'SBV')\n",
      "不 (171, 'ADV')\n",
      "会 (172, 'ADV')\n",
      "失去 (167, 'VOB')\n",
      "工作 (172, 'VOB')\n",
      "。 (18, 'WP')\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(word_list)):\n",
    "    print(word_list[i],parser_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载预训练好的词向量模型\n",
    "word2vec = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/limingxiao/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/limingxiao/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('表示', 0.8936804533004761),\n",
       " ('认为', 0.862716794013977),\n",
       " ('坦言', 0.8580939769744873),\n",
       " ('指出', 0.8195451498031616),\n",
       " ('看来', 0.7946385145187378),\n",
       " ('所说', 0.7787483930587769),\n",
       " ('透露', 0.7716749310493469),\n",
       " ('明说', 0.7267775535583496),\n",
       " ('称', 0.7238091826438904),\n",
       " ('介绍', 0.7030166387557983)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(\"说\",topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_word_list(word,top=10): \n",
    "    #取得与word最相近的10个词\n",
    "    word_list = []\n",
    "    for w in word2vec.most_similar(word,topn=top):\n",
    "        word_list.append(w[0])\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_similar_word(word): \n",
    "    #广度优先搜索 取得深度为10 与word 相近的所有词 \n",
    "    words = [word]\n",
    "    seen = set()\n",
    "    depth = 10\n",
    "    while words and depth > 0:\n",
    "        word = words.pop(0)\n",
    "        for w in word:\n",
    "            if w not in seen:\n",
    "                seen.add(w)\n",
    "                words.append(get_similar_word_list(w,10))\n",
    "            else:\n",
    "                continue\n",
    "        depth -= 1\n",
    "    return list(seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/limingxiao/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/limingxiao/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "spoken_words = gather_similar_word(\"说\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoken_words.append(\"报道\")  #人为加入未在预料中找到的表示说的词 因为这里我示范用的新闻和词向量训练的预料不同 所以手动加一下这个词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['眼中', '称', '说', '指出', '坦言', '明说', '写道', '看来', '地说', '所说', '透露', '普遍认为', '告诉', '眼里', '直言', '强调', '文说', '说道', '武说', '表示', '提到', '正说', '介绍', '相信', '认为', '问', '报道']\n"
     ]
    }
   ],
   "source": [
    "print(spoken_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get SBV relation in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_spoken_word_id_and_sub(spoken_words,word_list,parser_list):\n",
    "    #取得 新闻中 包含SBV关系 并且V表示的是说的意思 的主语和谓语的位置\n",
    "    id_list = []\n",
    "    for sub_id,parse_relation in enumerate(parser_list):\n",
    "        index,relation = parse_relation\n",
    "        if  relation == \"SBV\":\n",
    "            spoken_word = word_list[index-1]\n",
    "            if spoken_word in spoken_words:\n",
    "                word_id = index-1\n",
    "                id_list.append((sub_id,word_id))\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoken_words.append(\"报道\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=find_spoken_word_id_and_sub(spoken_words,word_list,parser_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3), (5, 17), (38, 39), (74, 76), (120, 126)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(news,word_list,idx):\n",
    "    #取得 说的内容 及SBV的宾语成分\n",
    "    # idx 为表示说的词在新闻中的位置信息\n",
    "    index = len(\"\".join(word_list[:idx+1]))\n",
    "    #寻找说这个词后 第一个句子 \n",
    "    stop1 = news[index+1:].find(\"。\")\n",
    "    stop2 = news[index+1:].find(\"!\")\n",
    "    stop3 = news[index+1:].find(\"?\")\n",
    "    #检查是不是后面没有句子了 \n",
    "    stop_list = [stop for stop in [stop1,stop2,stop3] if stop != -1]\n",
    "    if stop_list is None:\n",
    "        False\n",
    "    #返回后面的第一个句子\n",
    "    stop = min(stop_list)\n",
    "    \n",
    "    #检查表示说后面是否有双引号 引起来的句子\n",
    "    begin = 9999\n",
    "    end = 9999 \n",
    "    if \"“\" in news[index+1:] and \"”\" in news[index+1:]:\n",
    "        begin = news[index+1:].find(\"“\")\n",
    "        end = news[index+1:].find(\"”\")\n",
    "    \n",
    "    #第一种情况 双引号在stop前面，表示说词后面跟的是双引号的句子 则双引号里的内容即为说的内容\n",
    "    if begin < stop:\n",
    "        result = news[index+1+begin:index+1+end+1]\n",
    "    #第二种情况 双引号的内容在第一个句子后或无双引号，则说词后的句子即为说的内容。\n",
    "    else:\n",
    "        sentence = news[index+2:stop+index+2].strip()\n",
    "        result =sentence\n",
    "        sim = 1\n",
    "        next_id = stop+index+2\n",
    "        # 检查第二个句子是否也是说的内容，通过检查句子相似性来判断 若是相似度大于某个数值则表示 相近 这个句子也是说的内容\n",
    "        # 如果相似度大于0.7表示 该句话和前一句内容相似 所以这句话 也为说的内容 继续检查下一句话\n",
    "        while sim > 0.85 and next_id < len(news)-1:\n",
    "            next_sentence_id = next_id\n",
    "            if next_sentence_id < len(news)-1:\n",
    "                next_sentence,next_id = get_next_sentence(news,next_sentence_id)\n",
    "                sim = sentence_distance(sentence,next_sentence)\n",
    "            if sim > 0.85:\n",
    "                result += next_sentence\n",
    "                sentence = next_sentence\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_sentence(news,index):\n",
    "    #取得 index 后面的第一句话\n",
    "    stop1 = news[index+1:].find(\"。\")\n",
    "    stop2 = news[index+1:].find(\"!\")\n",
    "    stop3 = news[index+1:].find(\"?\")\n",
    "    stop_list = [stop for stop in [stop1,stop2,stop3] if stop != -1]\n",
    "    if stop_list is None:\n",
    "        False\n",
    "    stop = min(stop_list)\n",
    "    return news[index:index+stop+2],index+stop+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(sentence):\n",
    "    return \" \".join(jieba.cut(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_distance(sentence1,sentence2):\n",
    "    #计算句子间的距离 这里使用余弦距离 句子的embedding使用的是句子中词向量相加\n",
    "    word_list1 = cut(sentence1).split()\n",
    "    word_list2 = cut(sentence2).split()\n",
    "    \n",
    "    vec_1 = 0\n",
    "    vec_2 = 0\n",
    "    # get representation of sentence 1\n",
    "    for i in range(len(word_list1)):\n",
    "        if word_list1[i] in word2vec.wv.vocab:\n",
    "            vec_1 += word2vec.wv[word_list1[i]]\n",
    "        \n",
    "    # get representation of sentence 2\n",
    "    for j in range(len(word_list2)):\n",
    "        if word_list2[j] in word2vec.wv.vocab:\n",
    "            vec_2 += word2vec.wv[word_list2[j]]\n",
    "    \n",
    "    return np.dot(vec_1,vec_2)/(np.linalg.norm(vec_1)*np.linalg.norm(vec_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_and_view(idxs,news,word_list):\n",
    "    # 取得主语 以及说的内容\n",
    "    sub = []\n",
    "    speech = []\n",
    "    for sub_id,spoken_id in idxs:\n",
    "        sub.append(word_list[sub_id])\n",
    "        speech.append(get_sentence(news,word_list,spoken_id))\n",
    "    return sub,speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/c8/gq6b1ml94lx82wylsp11xt7c0000gn/T/jieba.cache\n",
      "Loading model cost 0.940 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "sub,speech = get_sub_and_view(idx,news,word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "彭博社 朗普正在开始为他的2020年竞选活动做准备。\n",
      "特朗普 “如果2020年除了我之外的任何人接管，将会出现前所未有的股市崩盘!”\n",
      "特朗普 “好吧，我想我伟大的飞行员会知道的。他们看到的情况与过去略有不同。我们会去看的。”\n",
      "杜特尔特 美国希望战争一直持续，这样其他国家就会购买美国的飞机、军舰和子弹。一旦战争结束，美国将有一大群人因此失业，并且无法在国外销售任何产品。\n",
      "杜特尔特 美国无法承受战争的结束，因为那样的话，很多美国人将失去工作,美国希望出售飞机、轮船和子弹。这就是美国希望一直能有战争，来保证美国人不会失去工作。\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sub)):\n",
    "    print(sub[i],speech[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
