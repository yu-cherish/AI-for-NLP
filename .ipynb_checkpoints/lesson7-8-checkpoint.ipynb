{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习的基本问题是：**利用模型对数据进行拟合**，学习的目的**并非**是对有限训练集进行正确预测，而是**对未曾出现的样本能够正确预测**。\n",
    "模型对训练集数据的误差称之为**经验误差**，对测试机的误差称之为**泛化误差**。模型对训练集意外的样本的预测能力就称之为模型的**泛化能力**。追求这种泛化能力始终是机器学习的目标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "过拟合和欠拟合是导致模型泛化能力不高的两种常见原因。都是模型的学习能力与数据的复杂度之间失配的结果。**“欠拟合”常常学习能力较弱，而数据复杂度较高的情况下出现，此时模型的学习能力不足，无法学习到数据集中的“一般规律”，因而导致泛化能力较弱。**与之相反，**“过拟合”常常在模型的学习能力过于强的情况中出现，此时模型的学习能力太强，以至于将训练集中单个样本自身的特点都能捕捉到，并将其认为是“一般规律”，同样这种情况也会导致模型泛化能力下降。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 欠拟合（underfitting-->hign bias）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 训练集和测试集的误差都很大，性能都较差，称之为欠拟合。\n",
    "2. 主要表现：输出结果的高偏差。\n",
    "3. 出现原因是：模型复杂度过低；特征量少"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解决方法：\n",
    "1. 增加新特征，可以考虑加入特征组合、高次特征，来增大假设空间\n",
    "2. 减少正则化参数，正则化的目的是用来防止过拟合的，但是模型出现了欠拟合，则需要减少正则化参数\n",
    "3. 使用非线性模型，比如核SVM 、决策树、深度学习等模型\n",
    "4. 调整模型的容量(capacity)，通俗地，模型的容量是指其拟合各种函数的能力\n",
    "5. 容量低的模型可能很难拟合训练集；使用集成学习方法，如Bagging ,将多个弱学习器Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 过拟合（overfitting --> high variance）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 能够较好的学习训练集数据的性质，而在测试机上性能较差。\n",
    "2. 主要表现：输出结果的高方差\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出现原因：\n",
    "1. 训练数据集样本单一，样本不足。如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型。\n",
    "2. 训练数据中噪声干扰过大。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系。\n",
    "3. 参数太多，模型过于复杂。模型太复杂，已经能够死记硬背记录下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。模型太复杂是过拟合的重要因素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解决方案：\n",
    "1. 正则化（Regularization）（L1和L2）\n",
    "2. 数据扩增，即增加训练数据样本\n",
    "3. Dropout\n",
    "4. Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化\n",
    "\n",
    "在模型训练的过程中，需要降低 loss 以达到提高 accuracy 的目的。此时，使用正则化之类的方法直接将权值的大小加入到 loss 里，在训练的时候限制权值变大。训练过程需要降低整体的 loss，这时候，一方面能降低实际输出与样本之间的误差，也能降低权值大小\n",
    "\n",
    "正则化方法包括 L0 正则、 L1 正则和 L2 正则，而正则一般是在目标函数之后加上对于的范数。但是在机器学习中一般使用 L2 正则:\n",
    "C=C0+λ2n.∑iw2i\n",
    "\n",
    "L2 范数是指向量各元素的平方和然后求平方根。可以使得 W 的每个元素都很小，都接近于0，但不会让它等于0，而是接近于0。 L2 正则项起到使得参数 W 变小加剧的效果，关于它为什么能防止过拟合简答的理解为：更小的参数值 W 意味着模型的复杂度更低，对训练数据的拟合刚刚好，不会过分拟合训练数据，从而使得不会过拟合，以提高模型的泛化能力\n",
    "\n",
    "### 数据扩增\n",
    "\n",
    "这是解决过拟合最有效的方法，只要给足够多的数据，让模型「看见」尽可能多的「例外情况」，它就会不断修正自己，从而得到更好的结果\n",
    "\n",
    "如何获取更多数据，可以有以下几个方法\n",
    "\n",
    "从数据源头获取更多数据\n",
    "根据当前数据集估计数据分布参数，使用该分布产生更多数据：这个一般不用，因为估计分布参数的过程也会代入抽样误差\n",
    "数据增强（Data Augmentation）：通过一定规则扩充数据。如在物体分类问题里，物体在图像中的位置、姿态、尺度，整体图片明暗度等都不会影响分类结果。我们就可以通过图像平移、翻转、缩放、切割等手段将数据库成倍扩充\n",
    "### Dropout\n",
    "\n",
    "在训练时，每次随机（如50%概率）忽略隐层的某些节点；这样，我们相当于随机从 2n(n个神经元的网络) 个模型中采样选择模型\n",
    "\n",
    "### Early stopping\n",
    "\n",
    "Early stopping便是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合\n",
    "\n",
    "具体做法是，在每一个Epoch结束时计算validation data的accuracy，当accuracy不再提高时，就停止训练。当然我们并不会在accuracy一降低的时候就停止训练，因为可能经过这个Epoch后，accuracy降低了，但是随后的Epoch又让accuracy又上去了，所以不能根据一两次的连续降低就判断不再提高。一般的做法是，在训练的过程中，记录到目前为止最好的validation accuracy，当连续10次Epoch（或者更多次）没达到最佳accuracy时，则可以认为accuracy不再提高了。此时便可以停止迭代了（Early Stopping）。这种策略也称为“No-improvement-in-n”，n即Epoch的次数，可以根据实际情况取，如10、20、30……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
